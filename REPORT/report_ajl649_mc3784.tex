%\documentclass[final,leqno,onefignum,onetabnum]{siamltexmm}
\documentclass[]{article}
\usepackage{amsmath}
\usepackage{paralist}

\usepackage{graphicx} % Allows including images

\title{An application of Spectral Clustering on Named Entity Recognition}

\author{Ali Josue Limon; Michele Cer\'u \\ 
\texttt{ajl649@nyu.edu }; \texttt{mc3784@nyu.edu}
}
\begin{document}
\maketitle
\newcommand{\slugmaster}{%
\slugger{siads}{xxxx}{xx}{x}{x---x}}%slugger should be set to juq, siads, sifin, or siims


%\newcommand{\slugmaster}{
%\slugger{siads}{xxxx}{xx}{x}{x---x}}%slugger should be set to juq, siads, sifin, or siims

\begin{abstract}
This study explores the peformance of spectral clustering in a Spanish NER task. Our approach comprises two main steps. Firstly, we represent each Entity in two different ways: 1) by the embedding vector formed from the entity and 2) by the embedding vector formed from the context and the entity.  Then, we use spectral clustering over the words embeddings  to analyze weather the words cluster according to their labels or not.  In order to train the word embedding, we used Word2Vec algorithm over a Spanish Billion Words Corpus with the skip-gram and CBOW models.  Our results suggest that 


\end{abstract}

\section{Introduction}


Named Entity Recognition (NER) is one of the important parts of NLP. It aims to find and classify expressions of special meaning in texts written in natural language.  These expressions can belong to different predefined classes such as Person, Organization and Location. \\

Prior work on NER come in three kinds: supervised learning algorithms, semi-supervised learning algorithms, and unsupervised learning algorithm. Compared with supervised and semi-supervised methods, unsupervised approach for entity recognition can overcome the difficulties on requirement of a large amount of labeled data, which is a common problem in some languages such as Spanish. \\

In this paper we investigate whether spectral clustering techniques can be successfully applied to NER in Spanish. In order to do so, we transform the original input, defined by the the word2Vector embeddings of entities and context, into a set of orthogonal eigenvectors. Then, We work in the space defined by the first few eigenvectors, using standard clustering techniques in the reduced space. \\

The paper is organized as follows. In Section 2, we provide a explanation of the data used to accomplish the study. Section 3 describes the architecture to create the word embeddings. Section 4 presents our framework for NER and Section 5 shows our experimental setting and results. Finally, Section 6 presents our final remarks.

\section{Data and Evaluation}
  The experimentation of this work will be carried on a corpora that consist of sentences extracted from news articles. The corpora corresponds to the \emph{CoNLL 2002} Shared Task Spanish data, the original source being the \emph{EFE Spanish Newswire Agency}.  The data consists of two columns separated by a single space. The first item on each line is a word and the second the named entity tag. In this dataset, there are four types of entities: person names (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC) and other (O). This dataset contains 11,752 sentences, 369,171 words and 26,706 Name Entities.  \\
 
  In order to create the word embedding, we added to the \emph{CoNLL 2002} dataset an unannotated corpus of the Spanish Billion Words Corpus [reference] with a total of 1420665810 raw words,  46925295 sentences and 3817833 unique tokens. Both Corpuses are preprocessed by replacing all non-alphanumeric characters with whitespaces, all numbers with the token ''DIGITO'' and all the multiple whitespaces with only one whitespace. \\

As evaluation method for the spectral cluster, we use Rand measure, which is a measure of the similarity between two data clusterings. Mas 




\section{Word2Vector}

Word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep nets can understand. \\ 

The purpose and usefulness of Word2vec is to group the vectors of similar words together in vectorspace. That is, it detects similarities mathematically. Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words. It does so without human intervention. \\

It does so in one of two ways, either using context to predict a target word (a method known as continuous bag of words, or CBOW), or using a word to predict a target context, which is called skip-gram. For both approaches a neural network is trained with a single hidden layer to perform a certain task depending of the approach. The goal is actually just to learn the weights of the hidden layer instead of use the neural network for the task we trained. These weights are actually the ?word vectors? that we?re trying to learn.

\section{Methodology}

To represent the context of a target word, we will use the vector formed by the continuous bag of words of the context \cite{vecSp}, that is the average of the word2vec \cite{word2vec} representation of all the words in the sentence containing a target word. In this way, the contribution of our project would be the implementation of the word embedding vector representation to compute spectral clustering. \\

\section{Experiments and Discussion}


\section{Conclusions}
  
\pagestyle{myheadings}
\thispagestyle{plain}


\begin{thebibliography}{1}
  \bibitem{dataset} http://www.cnts.ua.ac.be/conll2002/ner/
  \bibitem{spectral}http://www.cims.nyu.edu/$\sim$bandeira/TenLecturesFortyTwoProblems.pdf
  \bibitem{popescu} Popescu, M., \& Hristea, F. (2011). State of the art versus classical clustering for unsupervised word sense disambiguation. Artificial Intelligence Review, 35(3), 241?264. http://dx.doi.org/10.1007/s10462-010-9193-7.  
  \bibitem{vecSp} http://crscardellino.me/SBWCE/
   \bibitem{word2vec} Mikolov, T., Chen, K., Corrado, G., \& Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. arXiv preprint arXiv:1301.3781. 
   
\end{thebibliography} 


\end{document}


